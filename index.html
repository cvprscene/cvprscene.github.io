<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-121802070-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-121802070-1');
</script>
<meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
<style type="text/css">
a {
color: #000080;
text-decoration:none;
}

a:focus, a:hover {
color: #000080;
text-decoration:underline;
}

a.hover
{
    text-decoration: none;
}

 a.hover:hover
 {
    text-decoration: underline;
 }
body,td,th {
	font-family: Lora;
	font-size: 16px;
}
papertitle {
	font-family: Lora;
	font-size: 16px;
}
conference {
	font-family: Lora;
	font-size: 16px;
	color: #696969;
}
strong {
	font-family: Lora;
	font-size: 16px;
	font-weight: 600;
}
heading {
	font-family: Lora;
	color: #970024; /*#EB6400;*/
	font-size: 28px;
	font-weight: bold;
}
pvenue {
	display: inline-block;
	color: #500;
}
pkeywords{
	color: #EB6400;
}
</style>
    <!-- Custom Fonts -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
<link rel="icon" type="image/png" href="seal_icon.png">
<link href="css/styles.css" rel="stylesheet">
<title>Siyuan Huang - Statistics, UCLA</title>
<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
</head>
<body>
<div id="Banner">
    <div height="80" id="header" style="background-color:#000080; color: #000080">
      <center>
        <table width="1050" height="80" border="0">
          <tr>
		<td halign="center">
			<p align="center"><font size="6"><font color=#FFFFFF>Siyuan Huang</font> </p>
		</td>
	</tr>
        </table>
      </center>
    </div>
	<table width=1000 border="0" align="center" cellpadding="10">

	<tr>
		<td>
			<table width="100%" border="0" align="center" cellpadding="10">
			<tr>
							</td>
				<td width="15%" valign="top">
					<img src="img/self.png" alt="Seattle" width="100%" >
				</td>
				<td width="50%" valign="top">
			I am a third year Ph.D. student in the Department of Statistics at University of California, Los Angeles. I am currently doing computer vision research at the <a href="http://vcla.stat.ucla.edu">Center for Vision, Cognition, Learning, and Autonomy (VCLA)</a> and am advised by <a href="http://www.stat.ucla.edu/~sczhu/">Professor Song-Chun Zhu</a>. Before coming to UCLA, I graduated from <a href="http://www.tsinghua.edu.cn/publish/newthuen/">Tsinghua University</a> with a Bachelors in <a href="http://www.tsinghua.edu.cn/publish/auen/">Department of Automation</a>.</p>
			<br />My research interests lie on computer vision, machine learning and robotics. I currently focus on the problem of 3D scene understanding which contains 3D scene parsing, reconstruction and synthesis. I would like to develop tools to help machines learn 3D representations, percept 3D world, and interact with 3D environment from images or videos.
         
          <center>
          <a href=mailto:huangsiyuan@ucla.edu>E-Mail</a> / 
          <a href="files/cv.pdf">CV</a> / 
          <a href="https://scholar.google.com/citations?user=1NN7Ee8AAAAJ&hl=en&citsig=AMstHGQ_eDX956QVraGalRBAEWGG0ltzRw"> Google Scholar</a>
          </center>

			</tr>
			</table>
		</td>
	</tr>

	<tr>
		<td>
      <heading>News</heading>
      <ul>
      	<li> <font color="red"><strong>NEW</strong></font>&nbsp;&nbsp; 09/2018 One Paper accepted by NeurIPS 2018.
      	<li> 07/2018 One Paper accepted by ECCV 2018.
      	<li> 06/2018 One Paper accepted by IJCV 2018. </li>
		<li> 02/2018 One Paper accepted by CVPR 2018. </li>
        <li> 04/2017: One paper accepted by ICCV 2017. </li>
        <li> 09/2016: Started my Ph.D. at UCLA</li>
	</ul> <br /><br /><br />
     
   
      <heading>Publications</heading>

      <table width="110%" border="0" align="top" cellpadding="5">
      <tr>
				<td width="20%" valign="center">
					<img src="thumbnails/nips2018.png" alt="NeurIPS 2018" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>Cooperative Holisctic Scene Understanding: Unifying 3D Object, Layout, and Camera Pose Estimation</papertitle> <br />
					<strong>Siyuan Huang</strong>,
					<a href="http://web.cs.ucla.edu/~syqi/", style="color : #696969;">Siyuan Qi</a>,
					<a href="http://yolandaxiao.com/", style="color : #696969;">Yinxue Xiao</a>,
					<a href="http://www.yzhu.io/", style="color : #696969;">Yixin Zhu</a>,
					<a href="http://www.stat.ucla.edu/~ywu/", style="color : #696969;">Ying Nian Wu</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/", style="color : #696969;">Song-Chun Zhu</a> <br />
 					Neural Information Processing Systems (NeurIPS) 2018 <br />
 					<a href="files/nips2018cooperative.pdf">Paper</a> /
					<a href="files/nips2018cooperative_supp.pdf">Supplementary</a> /
					<a href="cooperative_parsing/poster.pdf"> Poster </a> /
					<a href="https://www.youtube.com/watch?v=kXCugGwnr68"> Video </a> /
					<a href="https://github.com/thusiyuan/cooperative_scene_parsing"> Code </a> /
					<a href="cooperative_parsing/main.html"> Project </a> <br />
					<pkeywords> Propose an end-to-end model that simultaneously solves tasks of 3D object detection, 3D layout estimation and camera pose estimation in real-time given only a single RGB image </pkeywords> <br />
					<br /> <br />
				</td>
			</tr>
		</table>

      <table width="110%" border="0" align="top" cellpadding="5">
      <tr>
				<td width="20%" valign="center">
					<img src="thumbnails/eccv2018.png" alt="ECCV 2018" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image</papertitle> <br />
					<strong>Siyuan Huang</strong>,
					<a href="http://web.cs.ucla.edu/~syqi/", style="color : #696969;">Siyuan Qi</a>,
					<a href="http://www.yzhu.io/", style="color : #696969;">Yixin Zhu</a>,
					<a href="http://yolandaxiao.com/", style="color : #696969;">Yinxue Xiao</a>,
					<a href="http://web.cs.ucla.edu/~yuanluxu/", style="color : #696969;">Yuanlu Xu</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/", style="color : #696969;">Song-Chun Zhu</a> <br />
 					European Conference on Computer Vision (ECCV) 2018 <br />
					<a href="files/eccv2018parsing.pdf">Paper</a> /
					<a href="files/eccv2018parsing_supp.pdf">Supplementary</a> /
					<a href="holistic_parsing/main.html"> Project </a> /
					<a href="https://github.com/thusiyuan/holistic_scene_parsing"> Code </a> / 
					<a href="holistic_parsing/poster.pdf"> Poster </a> /
					<a href="files/eccv2018.bib">Bibtex</a> <br />
					<pkeywords> Propose a computational framework to parse and reconstruct
the 3D configuration of an indoor scene from a single RGB image in an analysis-by-synthesis fasion using
a stochastic grammar model. </pkeywords> <br />
					<br /> <br />
				</td>
			</tr>
		</table>
      <table width="110%" border="0" align="center" cellpadding="5">
        <tr>
				<td width="20%" valign="top">
					<br />
					<img src="thumbnails/scenesynthesis17arxiv.gif" alt="IJCV" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle> 
          Configurable 3D Scene Synthesis and 2D Image Rendering
with Per-Pixel Ground Truth using Stochastic Grammars</a> </papertitle> <br />
          	<div class="authors">
          			<a href="http://www.seas.upenn.edu/~cffjiang/", style="color : #696969;">Chenfanfu Jiang</a> *,
					<a href="http://web.cs.ucla.edu/~syqi/", style="color : #696969;">Siyuan Qi</a> *,
					<a href="http://www.yzhu.io/", style="color : #696969;">Yixin Zhu</a> *,
					<strong>Siyuan Huang</strong> *, 
					<a href="http://jlin.crevado.com/", style="color : #696969;">Jenny Lin</a>,
					<a href="http://cpsc.yale.edu/people/xingwen-guo", style="color : #696969";>Xingwen Guo</a>,
					<a href="http://www.cs.umb.edu/~craigyu/", style="color : #696969;">Lap-Fai Yu</a>,
					<a href="http://web.cs.ucla.edu/~dt/", style="color : #696969;">Demetri Terzopoulos</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/", style="color : #696969;">Song-Chun Zhu</a>
			</div>
					* Equal contributions  <br />
 				  Internatianal Journal of Computer Vision (IJCV) 2018 <br />
					<a href="https://arxiv.org/pdf/1704.00112.pdf">Paper</a> /
					<a href="https://vimeo.com/211226594">Demo</a> <br />
					<pkeywords> Employ physics-based rendering to synthesize photorealistic RGB images while automatically synthesizing detailed,per-pixel ground truth data, including visible surface depth and normal, object identity and material information, as well as illumination.</pkeywords> <br />
					<br /><br />
				</td>
	    </tr></table>

     <table width="110%" border="0" align="center" cellpadding="5">
      <tr>
				<td width="20%" valign="top">
					<img src="thumbnails/cvpr2018synthesis.gif" alt="CVPR 2018" width="100%" class="border">
				</td>
				<td width="80%" valign="center">
					<papertitle>Human-centric Indoor Scene Synthesis using Stochastic Grammar</papertitle> <br />
					<a href="http://web.cs.ucla.edu/~syqi/", style="color : #696969;">Siyuan Qi</a>,
					<a href="http://www.yzhu.io/", style="color : #696969;">Yixin Zhu</a> ,
					<strong>Siyuan Huang</strong>,
					<a href="http://www.seas.upenn.edu/~cffjiang/", style="color : #696969;">Chenfanfu Jiang</a> ,
					<a href="http://www.stat.ucla.edu/~sczhu/", style="color : #696969;">Song-Chun Zhu</a> <br />
 					IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018 <br />
					<a href="files/cvpr2018synthesis.pdf">Paper</a> / 
					<a href="http://www.yzhu.io/projects/cvpr18_scenesynthesis/index.html"> Project </a> /
					<a href=https://github.com/SiyuanQi/human-centric-scene-synthesis> Code </a> /
					<a href="files/cvpr2018.bib">Bibtex</a>  <br />
					<pkeywords> Present a human-centric method to sample and synthesize 3D room layouts and 2D images thereof, for the purpose of obtaining large-scale 2D/3D image data with the perfect per-pixel ground truth.</pkeywords> <br />
					<br /> <br />
				</td>
			</tr>
			</table>
    
	    
      <table width="110%" border="0" align="center" cellpadding="5">
      <tr>
				<td width="20%" valign="top" >
					<img src="thumbnails/prediction.gif" alt="ICCV 2017" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle>Predicting Human Activities Using Stochastic Grammar</papertitle> <br />
					<a href="http://web.cs.ucla.edu/~syqi/", style="color : #696969;">Siyuan Qi</a>,
					<strong>Siyuan Huang</strong>,
					<a href="http://www.stat.ucla.edu/~ping.wei/", style="color : #696969;">Ping Wei</a>,
					<a href="http://www.stat.ucla.edu/~sczhu/", style="color : #696969;">Song-Chun Zhu</a> <br />
 					IEEE International Conference on Computer Vision (ICCV) 2017 <br />
					<a href="https://arxiv.org/pdf/1708.00945.pdf">Paper</a> / 
					<a href="files/iccv2017.bib">Bibtex</a> /
					<a href="https://github.com/SiyuanQi/grammar-activity-prediction">Code</a> <br />
					<pkeywords> Use a stochastic grammar model to capture the compositional structure of events, integrating human actions, objects, and their affordances for modeling the rich context between human and environment.</pkeywords> <br />
					<br /> <br />
				</td>
			</tr>
			</table>
<br />

      <table width="110%" border="0" align="center" cellpadding="5">
      <tr>
				<td width="20%" valign="center">
					<img src="thumbnails/arxiv15.png" alt="arXiv 2015" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle> Nonlinear Local Metric Learning for Person Re-identification</a> </papertitle> <br />
					<strong>Siyuan Huang</strong>,
					<a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/", style="color : #696969;">Jiwen Lu</a>,
					<a href="http://www.tsinghua.edu.cn/publish/auen/1713/2011/20110506105532098625469/20110506105532098625469_.html", style="color : #696969;">Jie Zhou</a>,
					<a href="https://www.cse.msu.edu/~jain/", style="color : #696969;">Anil K. Jain</a>
					<br />
 					arXiv 2015 <br />
 					<a href="https://arxiv.org/pdf/1511.05169.pdf">arXiv Paper</a> <br />
 					<pkeywords> Utilize the merits of both local metric learning and deep neural network to  exploit the complex nonlinear transformations in the feature space of person re-identification data.  </pkeywords>
					<br /> <br />
				</td>
			</tr>
      </table>

       <table width="110%" border="0" align="center" cellpadding="5">
      <tr>
				<td width="20%" valign="center">
					<img src="thumbnails/icip2015.png" alt="icip 2015" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle> Building Change Detection Based on 3D reconstruction</a> </papertitle> <br />
					Baohua Chen,
					<a href="http://ivg.au.tsinghua.edu.cn/people_phd.php", style="color : #696969;">Lei Deng</a>,
					<a href="http://ivg.au.tsinghua.edu.cn/people_phd.php", style="color : #696969;">Yueqi Duan</a>,
					<strong>Siyuan Huang</strong>,
					<a href="http://www.tsinghua.edu.cn/publish/auen/1713/2011/20110506105532098625469/20110506105532098625469_.html", style="color : #696969;">Jie Zhou</a>
					<br />
 					IEEE International Conference on Image Processing (ICIP) 2015 <br />
 					<a href="http://ieeexplore.ieee.org/document/7351582/">Paper</a> /
 					<a href="files/icip2015.bib">Bibtex</a> <br />
 					<pkeywords> Propose a change detection framework based on RGB-D map generated by 3D reconstruction which can overcome the large illumination changes .</pkeywords>
					<br /> <br />
				</td>
			</tr>
      </table>
       <!--
       <table width="100%" border="0" align="center" cellpadding="10">
      <tr>
				<td width="20%" valign="center">
					<img src="thumbnails/vcip15.png" alt="vcip 2015" width="100%" class="border">
				</td>
				<td width="80%" valign="top">
					<papertitle> Image Set Querying Based Localization</a> </papertitle> <br />
					<a href="http://ivg.au.tsinghua.edu.cn/people_phd.php", style="color : #696969;">Lei Deng</a> *,
					<strong>Siyuan Huang</strong> *,
					<a href="http://ivg.au.tsinghua.edu.cn/people_phd.php", style="color : #696969;">Yueqi Duan</a>,
					Baohua Chen,
					<a href="http://www.tsinghua.edu.cn/publish/auen/1713/2011/20110506105532098625469/20110506105532098625469_.html", style="color : #696969;">Jie Zhou</a>,
					<br />
					* Equal contributions 
					<br />
 					IEEE Visual Communications and Image Processing (VCIP) 2015 <br />
 					<a href="http://ieeexplore.ieee.org/document/7457924/">Paper</a> / 
 					<a href="files/vcip2015.bib">Bibtex</a> <br />
 					 					<pkeywords> Propose an approach to deal with the situation when a single image fail to localize in the image-set querying based system.  </pkeywords>
					<br /> <br />
				</td>
			</tr>
      </table>
		-->
		</td>
	</tr>	
	
	</table>

</body>

</html>
